# ğŸ“¡ Intelligent Task Offloading in Edge Computing using Deep Q-Learning

This project implements an intelligent task offloading mechanism using Deep Q-Learning (DQN) in a simulated edge computing environment. The network is created using Mininet, and a reinforcement learning agent built in PyTorch learns to decide whether to process a task locally or offload it to an edge server based on real-time network conditions and workload.

---

## ğŸ—ºï¸ Network Topology

The custom topology consists of:

- ğŸ‘¥ 6 User Equipments (UEs): ue1, ue2, ue3, ue4, ue5, ue6  
- ğŸ–¥ï¸ 3 Edge Servers: edge1, edge2, edge3  
- ğŸ”€ 1 Switch: s1  
- ğŸ® 1 Controller: c0  

ğŸ“¡ Bandwidth Configuration:

- UEs â†” Switch: 10 Mbps  
- Edge Servers â†” Switch: 100 Mbps  

ğŸ“· Topology Diagram:

<img src="./network_architecture.png" alt="Home Page" width="600"/>


---

## ğŸš€ Features

- âœ… Custom Mininet topology for edge computing
- âœ… Deep Q-Network (DQN) implementation using PyTorch
- âœ… Real-time RTT (Round-Trip Time) measurement using ping
- âœ… Dynamic and randomized task generation per UE
- âœ… Reward function based on latency (negative reward = high latency)
- âœ… Experience Replay Buffer for stable training
- âœ… Target Network synchronization for DQN stability
- âœ… CLI-based visualization of task offloading decisions

---

## ğŸ› ï¸ Prerequisites

Ensure the following software is installed:

- Linux OS (Tested on Ubuntu)
- Python 3.8 or above
- Mininet
- PyTorch
- NumPy

Install dependencies:

```bash```
pip install torch numpy

---
## ğŸ§  DQN Model Overview

The Deep Q-Network (DQN) makes decisions based on the following inputs and outputs to optimize task offloading in an edge computing environment:

ğŸ”¢ Input (State):

- RTT to edge1, edge2, and edge3  
- Task Size (range: 1MB to 10MB)  
- CPU Demand (range: 5% to 50%)  

ğŸ¯ Output (Action Space):

- 0 â†’ Process Locally  
- 1 â†’ Offload to edge1  
- 2 â†’ Offload to edge2  
- 3 â†’ Offload to edge3  

ğŸ Reward Function:

- Reward = âˆ’(total latency)  
- Lower latency results in a higher (less negative) reward  

ğŸ§® Model Architecture:

- Input Layer: 5 neurons (3 RTTs + task size + CPU demand)  
- Hidden Layer: 128 neurons (ReLU activation)  
- Output Layer: 4 Q-values (representing expected reward for each action)  

ğŸ“˜ Training Details:

- Total Episodes: 50  
- Tasks per Episode: 6 (one per UE)  
- Target Network Synchronization: Every 10 episodes  
- Experience Replay Buffer: Enabled to stabilize training  

---

## ğŸ“Š Result Analysis

- ğŸ“‰ Latency decreases over episodes as the agent learns optimal offloading strategies.  
- ğŸ“ˆ Edge servers are selected when the combination of RTT and CPU cost is lower than local computation.  
- ğŸ¤– The DQN agent adapts dynamically to varying task sizes and CPU loads.  
- âš™ï¸ CPU % reflects the computational workload of each task, directly affecting latency for both local and edge execution.
