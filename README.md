# ğŸ“¡ Intelligent Task Offloading in Edge Computing using Deep Q-Learning

This project implements an intelligent task offloading mechanism using Deep Q-Learning (DQN) in a simulated edge computing environment. The network is built using Mininet, and PyTorch is used to train a reinforcement learning agent that dynamically decides where to process user tasks â€” locally or on one of several edge servers.

---

## ğŸ—ºï¸ Network Topology

The custom topology consists of:

- 6 User Equipments (UEs): ue1, ue2, ue3, ue4, ue5, ue6
- 3 Edge Servers: edge1, edge2, edge3
- 1 Switch: s1
- 1 Controller: c0

ğŸ’¡ Bandwidth:
- UEs â†’ Switch: 10 Mbps
- Edge Servers â†’ Switch: 100 Mbps

ğŸ“· Topology Diagram:

![Topology](https://raw.githubusercontent.com/Savita-Sidnal/Networking-Minor-Project/main/images/topology.png)

---

## ğŸš€ Features

- âœ… Mininet Custom Topology
- âœ… Deep Q-Network (DQN) using PyTorch
- âœ… Real-time RTT measurement using ping
- âœ… Dynamic task generation per UE
- âœ… Reward computation based on latency
- âœ… Experience Replay Buffer
- âœ… Target Network Synchronization
- âœ… Command-line visualization of offloading decisions

---

## ğŸ› ï¸ Prerequisites

- OS: Linux (tested on Ubuntu)
- Python 3.8+
- Mininet
- PyTorch
- NumPy

Install dependencies using:

```bash```
pip install torch numpy

## ğŸ§  DQN Overview

- Input (State):
  - RTT to edge1, edge2, edge3
  - Task Size (1MBâ€“10MB)
  - CPU Demand (5%â€“50%)

- Output (Action Space):
  - 0: Process locally
  - 1: Offload to edge1
  - 2: Offload to edge2
  - 3: Offload to edge3

- Reward:
  - Negative of total latency (smaller latency â†’ higher reward)

- Architecture:
  - Input layer: 5 neurons
  - Hidden layer: 128 ReLU
  - Output layer: 4 Q-values

- Training:
  - 50 episodes with 6 UEs per episode
  - Experience Replay Buffer
  - Target Network update every 10 episodes

---
## ğŸ“Š Result Analysis

- ğŸ“‰ Latency reduces over episodes as the agent learns optimal offloading.
- ğŸ“ˆ Edge servers are preferred when RTT + CPU cost < local processing.
- ğŸ§  DQN learns adaptively using feedback from network measurements.
- âš™ï¸ CPU % represents the workload for each task and influences the total latency during both local and edge execution.

--- 
